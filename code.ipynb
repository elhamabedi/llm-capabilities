{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16f5c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[+] Generating stories using model: phi3:3.8b\n",
      "============================================================\n",
      "\n",
      "[1] Prompt: Write a short Horror story about space featuring a intelligent robot.\n",
      " Story generated successfully.\n",
      "\n",
      "[2] Prompt: Generate a Horror tale involving a young scientist and zombie apocalypse.\n",
      " Story generated successfully.\n",
      "\n",
      "[3] Prompt: Write a short Science Fiction story about space featuring a brave explorer.\n",
      " Story generated successfully.\n",
      "\n",
      "[4] Prompt: Write a short Historical story about desert island featuring a wise wizard.\n",
      " Story generated successfully.\n",
      "\n",
      "[5] Prompt: Create a Science Fiction narrative where a wise wizard encounters desert island.\n",
      " Story generated successfully.\n",
      "\n",
      "[6] Prompt: Write a short Romance story about hidden treasure featuring a young scientist.\n",
      " Story generated successfully.\n",
      "\n",
      "[7] Prompt: Write a short Crime story about hidden treasure featuring a intelligent robot.\n",
      " Story generated successfully.\n",
      "\n",
      "[8] Prompt: Write a short Historical story about space featuring a wise wizard.\n",
      " Story generated successfully.\n",
      "\n",
      "[9] Prompt: Generate a Mystery tale involving a brave explorer and hidden treasure.\n",
      " Story generated successfully.\n",
      "\n",
      "[10] Prompt: Write a short Romance story about hidden treasure featuring a young scientist.\n",
      " Story generated successfully.\n",
      "\n",
      "============================================================\n",
      "[+] Generating stories using model: qwen2.5vl:3b\n",
      "============================================================\n",
      "\n",
      "[1] Prompt: Write a short Adventure story about time machine featuring a intelligent robot.\n",
      " Story generated successfully.\n",
      "\n",
      "[2] Prompt: Generate a Romance tale involving a young scientist and hidden treasure.\n",
      " Story generated successfully.\n",
      "\n",
      "[3] Prompt: Create a Historical narrative where a brave explorer encounters desert island.\n",
      " Story generated successfully.\n",
      "\n",
      "[4] Prompt: Write a short Historical story about desert island featuring a intelligent robot.\n",
      " Story generated successfully.\n",
      "\n",
      "[5] Prompt: Write a short Crime story about time machine featuring a intelligent robot.\n",
      " Story generated successfully.\n",
      "\n",
      "[6] Prompt: Generate a Romance tale involving a wise wizard and time machine.\n",
      " Story generated successfully.\n",
      "\n",
      "[7] Prompt: Generate a Adventure tale involving a brave explorer and desert island.\n",
      " Story generated successfully.\n",
      "\n",
      "[8] Prompt: Create a Mystery narrative where a wise wizard encounters time machine.\n",
      " Story generated successfully.\n",
      "\n",
      "[9] Prompt: Write a short Historical story about time machine featuring a young scientist.\n",
      " Story generated successfully.\n",
      "\n",
      "[10] Prompt: Create a Comedy narrative where a young scientist encounters time machine.\n",
      " Story generated successfully.\n",
      "\n",
      " Results saved to json_outputs\\asg_output.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Automatic Story Generation (ASG) - NLP Assignment 4, Section 3.1\n",
    "Improved with Few-Shot Prompting\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# API endpoint for Ollama\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# List of genres for story generation\n",
    "GENRES = [\n",
    "    \"Science Fiction\", \"Horror\", \"Comedy\", \"Romance\", \"Adventure\",\n",
    "    \"Crime\", \"Fantasy\", \"Short Story\", \"Historical\", \"Mystery\"\n",
    "]\n",
    "\n",
    "# Sample topics and characters\n",
    "TOPICS = [\"space\", \"desert island\", \"hidden treasure\", \"time machine\", \"zombie apocalypse\"]\n",
    "CHARACTERS = [\"young scientist\", \"brave explorer\", \"intelligent robot\", \"wise wizard\"]\n",
    "\n",
    "# Decoding hyperparameters for generation diversity\n",
    "DECODING_PARAMS = {\n",
    "    \"temperature\": [0.7, 0.9, 1.2],   # Controls randomness in output\n",
    "    \"top_p\": [0.9, 0.95]              # Nucleus sampling threshold\n",
    "}\n",
    "\n",
    "# Few-shot examples for each genre\n",
    "FEW_SHOT_EXAMPLES = {\n",
    "    \"Science Fiction\": {\n",
    "        \"topic\": \"space\",\n",
    "        \"story\": \"In the year 2145, a spaceship named Noah traveled to distant stars. The crew found a strange device that could communicate with unknown civilizations.\"\n",
    "    },\n",
    "    \"Horror\": {\n",
    "        \"topic\": \"desert island\",\n",
    "        \"story\": \"The sun had long since dipped into the horizon, casting a warm orange glow over the sandy beach of the deserted island... until the screams began.\"\n",
    "    },\n",
    "    \"Comedy\": {\n",
    "        \"topic\": \"hidden treasure\",\n",
    "        \"story\": \"Once upon a time in a distant village, there was a rumor that treasure hunters found a hidden fortune... but it turned out to be a giant chocolate bar.\"\n",
    "    },\n",
    "    \"Romance\": {\n",
    "        \"topic\": \"time machine\",\n",
    "        \"story\": \"Amidst the hum of the time machine, Dr. Emma Taylor and Captain Liam Chen found themselves in 1920s Paris. Their scientific mission quickly turned into a love story.\"\n",
    "    },\n",
    "    \"Adventure\": {\n",
    "        \"topic\": \"space\",\n",
    "        \"story\": \"Once upon a time, there was a young astronaut named Alex who dreamed of exploring the vastness of space...\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prompt templates for story generation\n",
    "PROMPT_TEMPLATES = [\n",
    "    \"Write a short {genre} story about {topic} featuring a {character}.\",\n",
    "    \"Create a {genre} narrative where a {character} encounters {topic}.\",\n",
    "    \"Generate a {genre} tale involving a {character} and {topic}.\"\n",
    "]\n",
    "\n",
    "def generate_story(model_name, genre, topic, character, prompt_template):\n",
    "    \"\"\"\n",
    "    Generates a story using few-shot prompting to guide the LLM.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): Name of the LLM to use\n",
    "        genre (str): Story genre (e.g., Horror, Comedy)\n",
    "        topic (str): Story topic (e.g., space, time machine)\n",
    "        character (str): Main character of the story\n",
    "        prompt_template (str): Template to generate the prompt\n",
    "\n",
    "    Returns:\n",
    "        str: Generated story or None if failed\n",
    "    \"\"\"\n",
    "    # Get a few-shot example for the selected genre\n",
    "    few_shot = FEW_SHOT_EXAMPLES.get(genre, FEW_SHOT_EXAMPLES[\"Science Fiction\"])\n",
    "\n",
    "    # Create the prompt using the selected template\n",
    "    prompt = prompt_template.format(genre=genre, topic=topic, character=character)\n",
    "\n",
    "    # Enhance the prompt with few-shot example\n",
    "    full_prompt = f\"\"\"\n",
    "You are to generate a short {genre} story about {topic}, featuring {character}.\n",
    "\n",
    "Few-Shot Example:\n",
    "Genre: {few_shot['topic']}\n",
    "Story: {few_shot['story']}\n",
    "\n",
    "Now, write your own story using the following prompt:\n",
    "{prompt}\n",
    "\n",
    "Story:\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": full_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": random.choice(DECODING_PARAMS[\"temperature\"]),\n",
    "            \"top_p\": random.choice(DECODING_PARAMS[\"top_p\"])\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['response'].strip()\n",
    "        else:\n",
    "            print(f\"[Error] Failed to get response from {model_name}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[Exception] {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_asg(models=[\"phi3:3.8b\", \"qwen2.5vl:3b\"]):\n",
    "    \"\"\"\n",
    "    Main function to generate stories using few-shot prompting.\n",
    "\n",
    "    Parameters:\n",
    "        models (list): List of LLMs to use for story generation\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing metadata and stories\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\n{'='*60}\\n[+] Generating stories using model: {model}\\n{'='*60}\")\n",
    "        for i in range(10):  # Generate 10 stories per model\n",
    "            genre = random.choice(GENRES)\n",
    "            topic = random.choice(TOPICS)\n",
    "            character = random.choice(CHARACTERS)\n",
    "            prompt_template = random.choice(PROMPT_TEMPLATES)\n",
    "\n",
    "            prompt = prompt_template.format(genre=genre, topic=topic, character=character)\n",
    "\n",
    "            print(f\"\\n[{i+1}] Prompt: {prompt}\")\n",
    "\n",
    "            story = generate_story(model, genre, topic, character, prompt_template)\n",
    "            if story:\n",
    "                result = {\n",
    "                    \"model\": model,\n",
    "                    \"story_id\": i + 1,\n",
    "                    \"genre\": genre,\n",
    "                    \"topic\": topic,\n",
    "                    \"character\": character,\n",
    "                    \"prompt_used\": prompt,\n",
    "                    \"few_shot_used\": FEW_SHOT_EXAMPLES.get(genre, FEW_SHOT_EXAMPLES[\"Science Fiction\"])[\"story\"],\n",
    "                    \"story\": story\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(\" Story generated successfully.\")\n",
    "            else:\n",
    "                print(\" Failed to generate story.\")\n",
    "\n",
    "            time.sleep(2)  # Small delay between generations\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results(results, filename=\"asg_output.json\"):\n",
    "    \"\"\"\n",
    "    Saves the generated stories into a JSON file inside the 'json_outputs' folder.\n",
    "\n",
    "    Parameters:\n",
    "        results (list): List of story dictionaries\n",
    "        filename (str): Name of the output JSON file\n",
    "    \"\"\"\n",
    "    os.makedirs(\"json_outputs\", exist_ok=True)\n",
    "    filepath = os.path.join(\"json_outputs\", filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n Results saved to {filepath}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)  # Set seed for reproducibility\n",
    "\n",
    "    output = run_asg()\n",
    "    save_results(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa3b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Story ID: 1, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Horror story about space featuring a intelligent robot.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 2, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Generate a Horror tale involving a young scientist and zombie apocalypse.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 3, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Science Fiction story about space featuring a brave explorer.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 4, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Historical story about desert island featuring a wise wizard.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 5, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Create a Science Fiction narrative where a wise wizard encounters desert island.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 6, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Romance story about hidden treasure featuring a young scientist.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 7, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Crime story about hidden treasure featuring a intelligent robot.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 8, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Historical story about space featuring a wise wizard.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 9, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Generate a Mystery tale involving a brave explorer and hidden treasure.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 10, Model: phi3:3.8b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Romance story about hidden treasure featuring a young scientist.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 1, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Adventure story about time machine featuring a intelligent robot.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 2, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Generate a Romance tale involving a young scientist and hidden treasure.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 3, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Create a Historical narrative where a brave explorer encounters desert island.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 4, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Historical story about desert island featuring a intelligent robot.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 5, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Crime story about time machine featuring a intelligent robot.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 6, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Generate a Romance tale involving a wise wizard and time machine.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 7, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Generate a Adventure tale involving a brave explorer and desert island.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 8, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Create a Mystery narrative where a wise wizard encounters time machine.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 9, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Write a short Historical story about time machine featuring a young scientist.\"\n",
      " Summary generated successfully.\n",
      "\n",
      "[Story ID: 10, Model: qwen2.5vl:3b]\n",
      "Prompt used for summarization:\n",
      "\"Create a Comedy narrative where a young scientist encounters time machine.\"\n",
      " Summary generated successfully.\n",
      "\n",
      " Summaries saved to json_outputs/ats_output.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Abstractive Text Summarization (ATS), Section 3.2\n",
    "Improved with Chain-of-Thought (CoT) and Few-Shot Prompting\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Ollama API settings\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# List of models used\n",
    "# Changed from [\"llama3.2:1b\", \"qwen2.5vl:3b\"] to use phi3:3.8b instead of llama3.2:1b\n",
    "MODELS = [\"phi3:3.8b\", \"qwen2.5vl:3b\"]\n",
    "\n",
    "# Few-Shot Examples for summarization\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"story\": \"In the year 2145, humanity had finally cracked the code to interstellar travel...\",\n",
    "        \"summary\": \"A spaceship crew discovers an alien probe that has been waiting for 1000 years to send a message back to Earth.\"\n",
    "    },\n",
    "    {\n",
    "        \"story\": \"Once upon a time in a distant village, there was a rumor going around that a group of treasure hunters had stumbled upon a hidden fortune...\",\n",
    "        \"summary\": \"Treasure hunters discover a hidden chocolate bar, proving that the real treasure was their friendship all along.\"\n",
    "    },\n",
    "    {\n",
    "        \"story\": \"Dr. Emma Taylor and Captain Liam Chen found themselves in 1920s Paris...\",\n",
    "        \"summary\": \"A scientific mission quickly turns into a love story in 1920s Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prompt templates with CoT + Few-Shot\n",
    "SUMMARY_PROMPT_TEMPLATES = [\n",
    "    \"\"\"\n",
    "    Step-by-Step Reasoning:\n",
    "    1. Read the following story carefully.\n",
    "    2. Identify the main plot and key events.\n",
    "    3. Paraphrase the story in your own words.\n",
    "    4. Write a concise one-sentence summary.\n",
    "\n",
    "    Few-Shot Example:\n",
    "    Story: {few_shot_story}\n",
    "    Summary: {few_shot_summary}\n",
    "\n",
    "    Story:\n",
    "    {story}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Generate a one-sentence summary of the following story. Do not copy sentences directly from the original text.\n",
    "\n",
    "    Few-Shot Example:\n",
    "    Story: {few_shot_story}\n",
    "    Summary: {few_shot_summary}\n",
    "\n",
    "    Story:\n",
    "    {story}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Write a short, creative summary that captures the essence of this story without copying sentences.\n",
    "\n",
    "    Step-by-Step:\n",
    "    - Understand the main events\n",
    "    - Rephrase in new words\n",
    "    - Keep it under 50 words\n",
    "\n",
    "    Few-Shot Example:\n",
    "    Story: {few_shot_story}\n",
    "    Summary: {few_shot_summary}\n",
    "\n",
    "    Story:\n",
    "    {story}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Decoding hyperparameters\n",
    "DECODING_PARAMS = {\n",
    "    \"temperature\": [0.7, 0.9],   # Lower means more deterministic output\n",
    "    \"top_p\": [0.9, 0.95]         # Nucleus sampling threshold\n",
    "}\n",
    "\n",
    "def generate_summary(model_name, story_text):\n",
    "    \"\"\"\n",
    "    Calls Ollama API to generate a summary from a given story.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): Name of the LLM to use\n",
    "        story_text (str): The full story text to summarize\n",
    "    \n",
    "    Returns:\n",
    "        dict: Generated summary, inference time, and validity status\n",
    "    \"\"\"\n",
    "    # Select a few-shot example\n",
    "    few_shot = random.choice(FEW_SHOT_EXAMPLES)\n",
    "    few_shot_story = few_shot[\"story\"]\n",
    "    few_shot_summary = few_shot[\"summary\"]\n",
    "\n",
    "    # Select a prompt template and format it\n",
    "    prompt_template = random.choice(SUMMARY_PROMPT_TEMPLATES)\n",
    "    prompt = prompt_template.format(\n",
    "        story=story_text,\n",
    "        few_shot_story=few_shot_story,\n",
    "        few_shot_summary=few_shot_summary\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": random.choice(DECODING_PARAMS[\"temperature\"]),\n",
    "            \"top_p\": random.choice(DECODING_PARAMS[\"top_p\"]),\n",
    "            \"max_tokens\": 100,\n",
    "            \"stop\": [\"\\n\\n\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        inference_time = round(time.time() - start_time, 2)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            summary = response.json()['response'].strip()\n",
    "\n",
    "            # Check if summary is valid\n",
    "            is_valid = len(summary.split()) > 10 and not any(\n",
    "                prompt in summary for prompt in [\"Here is a one-sentence summary\", \"Here is a concise summary\"]\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"summary\": summary,\n",
    "                \"inference_time\": inference_time,\n",
    "                \"is_valid\": is_valid\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"summary\": None,\n",
    "                \"inference_time\": round(time.time() - start_time, 2),\n",
    "                \"is_valid\": False\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"summary\": f\"[Error] {e}\",\n",
    "            \"inference_time\": round(time.time() - start_time, 2),\n",
    "            \"is_valid\": False\n",
    "        }\n",
    "\n",
    "\n",
    "def run_summarization(input_file=\"json_outputs/asg_output.json\", output_file=\"json_outputs/ats_output.json\"):\n",
    "    \"\"\"\n",
    "    Main function to process all stories and generate summaries.\n",
    "    \"\"\"\n",
    "    # Load stories from ASG output\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            stories = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[Error] File '{input_file}' not found. Make sure you have run ASG and saved output in 'json_outputs/asg_output.json'.\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for story in stories:\n",
    "        print(f\"\\n[Story ID: {story['story_id']}, Model: {story['model']}]\")\n",
    "        print(\"Prompt used for summarization:\")\n",
    "        print(f\"\\\"{story.get('prompt_used', 'No prompt used')}\\\"\")  \n",
    "\n",
    "        summary_data = generate_summary(story[\"model\"], story[\"story\"])\n",
    "\n",
    "        result = {\n",
    "            \"original_model\": story[\"model\"],\n",
    "            \"story_id\": story[\"story_id\"],\n",
    "            \"genre\": story[\"genre\"],\n",
    "            \"topic\": story[\"topic\"],\n",
    "            \"original_prompt\": story.get(\"prompt_used\", \"\"),  \n",
    "            \"original_story\": story[\"story\"],\n",
    "            \"summary\": summary_data[\"summary\"],\n",
    "            \"inference_time\": summary_data[\"inference_time\"],\n",
    "            \"is_valid\": summary_data[\"is_valid\"]\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        if summary_data[\"is_valid\"]:\n",
    "            print(\" Summary generated successfully.\")\n",
    "        else:\n",
    "            print(\" Invalid or incomplete summary.\")\n",
    "\n",
    "        time.sleep(2)  \n",
    "\n",
    "    # Save results to JSON\n",
    "    os.makedirs(\"json_outputs\", exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n Summaries saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)  # Set seed for reproducibility\n",
    "    run_summarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f883d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded 100 items from the dataset.\n",
      "\n",
      "======================================================================\n",
      "[+] Running NLI Classification using model: phi3:3.8b on FULL dataset (100 examples)\n",
      "======================================================================\n",
      "\n",
      "[001/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[002/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[003/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[004/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[005/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[006/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[007/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[008/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[009/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[010/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[011/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[012/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[013/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[014/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[015/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[016/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[017/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[018/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[019/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[020/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[021/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[022/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[023/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[024/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[025/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[026/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[027/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: entails) - Incorrect\n",
      "\n",
      "[028/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[029/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[030/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: neutral) - Incorrect\n",
      "\n",
      "[031/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[032/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[033/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: neutral) - Incorrect\n",
      "\n",
      "[034/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[035/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[036/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[037/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[038/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[039/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[040/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[041/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[042/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[043/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[044/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[045/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[046/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[047/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: entails) - Incorrect\n",
      "\n",
      "[048/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[049/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[050/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[051/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[052/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[053/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[054/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[055/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[056/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[057/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[058/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[059/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[060/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[061/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[062/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[063/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[064/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[065/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[066/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[067/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[068/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[069/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[070/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[071/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[072/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[073/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[074/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[075/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[076/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: neutral) - Incorrect\n",
      "\n",
      "[077/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[078/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[079/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[080/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[081/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[082/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[083/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[084/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[085/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[086/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[087/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[088/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[089/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[090/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[091/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[092/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[093/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[094/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[095/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: neutral) - Incorrect\n",
      "\n",
      "[096/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[097/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[098/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: neutral) - Incorrect\n",
      "\n",
      "[099/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[100/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "--- Summary for Model: phi3:3.8b (Full Dataset, Standard Approach) ---\n",
      "Accuracy (on valid predictions): 0.7900\n",
      "Reject Rate: 0.0000\n",
      "Average Inference Time: 6.29 seconds\n",
      "Valid Predictions: 100/100\n",
      "\n",
      "[Success] NLI predictions and metrics saved to 'json_outputs/nli_output_phi3.json'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Natural Language Inference (NLI) - NLP Assignment 4, Section 3.3\n",
    "Running Classification using Phi3.3.8b\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ollama API endpoint\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Model to be used for NLI task\n",
    "# As per assignment instructions, this is one of the two required models.\n",
    "# Phi3.3.8b was selected as it showed good performance in initial tests.\n",
    "MODELS = [\"phi3:3.8b\"]\n",
    "\n",
    "# Standard labels expected from the LLM for the NLI task\n",
    "MODEL_NLI_LABELS = [\"entailment\", \"contradiction\", \"neutral\"]\n",
    "\n",
    "# Mapping to normalize labels from the dataset to match model outputs.\n",
    "# The dataset might use 'entails'/'contradicts', while the model outputs 'entailment'/'contradiction'.\n",
    "# This ensures accurate calculation of accuracy.\n",
    "LABEL_MAPPING = {\n",
    "    \"entails\": \"entailment\",\n",
    "    \"entailment\": \"entailment\",\n",
    "    \"contradicts\": \"contradiction\",\n",
    "    \"contradiction\": \"contradiction\",\n",
    "    \"neutral\": \"neutral\"\n",
    "}\n",
    "\n",
    "# Prompt template for the NLI task.\n",
    "# Provides a clear definition of the task and expects a single-word answer.\n",
    "STANDARD_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert at NLI (Natural Language Inference). Given a premise and a hypothesis, determine if the relationship is:\n",
    "- entailment (the hypothesis must be true if the premise is true)\n",
    "- contradiction (the hypothesis must be false if the premise is true)\n",
    "- neutral (the hypothesis is neither entailed nor contradicted by the premise)\n",
    "\n",
    "Answer ONLY with one of these three words: entailment, contradiction, neutral\n",
    "\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "Relationship:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Decoding parameters to ensure consistent and concise outputs for classification.\n",
    "# Low temperature for determinism, stop tokens to prevent extra text.\n",
    "STANDARD_DECODING_PARAMS = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 15,\n",
    "    \"stop\": [\"\\n\", \".\", \" \"]\n",
    "}\n",
    "\n",
    "def classify_nli_standard(model_name, premise, hypothesis):\n",
    "    \"\"\"\n",
    "    Classifies the relationship between a premise and hypothesis using an LLM via Ollama API.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the LLM to use (e.g., \"phi3:3.8b\").\n",
    "        premise (str): The premise sentence.\n",
    "        hypothesis (str): The hypothesis sentence.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the predicted label, raw model output,\n",
    "              inference time, and validity status.\n",
    "              - predicted_label (str): The extracted NLI label (entailment, contradiction, neutral) or raw output if invalid.\n",
    "              - raw_output (str): The complete, unprocessed text returned by the model.\n",
    "              - inference_time (float): The time taken for the model to respond, in seconds.\n",
    "              - is_valid (bool): True if a valid label was successfully extracted, False otherwise.\n",
    "    \"\"\"\n",
    "    # Format the prompt with the current premise and hypothesis\n",
    "    prompt = STANDARD_PROMPT_TEMPLATE.format(premise=premise, hypothesis=hypothesis)\n",
    "\n",
    "    # Prepare the payload for the Ollama API request\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": STANDARD_DECODING_PARAMS\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Send the request to the Ollama API\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        inference_time = round(time.time() - start_time, 2)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            raw_output = response.json()['response'].strip().lower()\n",
    "            \n",
    "            # --- Parsing Logic ---\n",
    "            # 1. Look for the label at the very beginning of the output (most reliable)\n",
    "            predicted_label = None\n",
    "            for label in MODEL_NLI_LABELS:\n",
    "                if raw_output.startswith(label):\n",
    "                    predicted_label = label\n",
    "                    break\n",
    "\n",
    "            # 2. If not found at the start, check if the label appears anywhere in the output (fallback)\n",
    "            if not predicted_label:\n",
    "                 for label in MODEL_NLI_LABELS:\n",
    "                     if label in raw_output:\n",
    "                         predicted_label = label\n",
    "                         break\n",
    "            \n",
    "            # 3. Determine if the prediction is valid (i.e., a label was found)\n",
    "            is_valid = predicted_label is not None\n",
    "            \n",
    "            # 4. If no valid label was found, store the raw output for debugging\n",
    "            if not is_valid:\n",
    "                predicted_label = raw_output \n",
    "\n",
    "            return {\n",
    "                \"predicted_label\": predicted_label,\n",
    "                \"raw_output\": raw_output,\n",
    "                \"inference_time\": inference_time,\n",
    "                \"is_valid\": is_valid\n",
    "            }\n",
    "        else:\n",
    "            # Handle unsuccessful API request\n",
    "            return {\n",
    "                \"predicted_label\": None,\n",
    "                \"raw_output\": f\"[Error] Status {response.status_code}\",\n",
    "                \"inference_time\": inference_time,\n",
    "                \"is_valid\": False\n",
    "            }\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions during the API call\n",
    "        return {\n",
    "            \"predicted_label\": None,\n",
    "            \"raw_output\": f\"[Exception] {e}\",\n",
    "            \"inference_time\": round(time.time() - start_time, 2),\n",
    "            \"is_valid\": False\n",
    "        }\n",
    "\n",
    "\n",
    "def run_nli_classification_standard(input_file=\"datasets/nli/nli.csv\", output_file=\"json_outputs/nli_output_phi3.json\"):\n",
    "    \"\"\"\n",
    "    Main function to load the full NLI dataset, run classification for the specified model,\n",
    "    calculate performance metrics, and save the detailed results.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file containing the NLI data (premise, hypothesis, label).\n",
    "                          As per the assignment PDF, this truncated dataset contains exactly 100 items.\n",
    "        output_file (str): Path to the output JSON file where results and metrics will be saved.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        # Load the NLI data from the provided CSV file\n",
    "        df = pd.read_csv(input_file)\n",
    "        # Convert the DataFrame to a list of dictionaries for easy iteration\n",
    "        nli_data = df.to_dict('records')\n",
    "        print(f\"[Info] Loaded {len(nli_data)} items from the dataset.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[Error] File '{input_file}' not found. Please check the file path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read the CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # This will store the final results for all models (though we are only using one here)\n",
    "    all_results = []\n",
    "\n",
    "    # --- 2. Run Classification for Each Model ---\n",
    "    for model in MODELS:\n",
    "        print(f\"\\n{'='*70}\\n[+] Running NLI Classification using model: {model} on FULL dataset ({len(nli_data)} examples)\\n{'='*70}\")\n",
    "        \n",
    "        # Store results and counters for this specific model\n",
    "        model_results = []\n",
    "        correct_predictions = 0\n",
    "        total_valid = 0\n",
    "        total_inference_time = 0.0  # For calculating average inference time\n",
    "        \n",
    "        # --- 3. Iterate Through Dataset ---\n",
    "        for i, item in enumerate(nli_data):\n",
    "            # Extract data for the current item\n",
    "            premise = item[\"premise\"]\n",
    "            hypothesis = item[\"hypothesis\"]\n",
    "            true_label_raw = item[\"label\"] \n",
    "            \n",
    "            # Normalize the true label from the dataset to match our standard labels\n",
    "            true_label_normalized = LABEL_MAPPING.get(true_label_raw, true_label_raw)\n",
    "\n",
    "            print(f\"\\n[{i+1:03d}/{len(nli_data)}] Classifying...\")\n",
    "\n",
    "            # Get the model's prediction\n",
    "            prediction_data = classify_nli_standard(model, premise, hypothesis)\n",
    "            \n",
    "            # Store detailed information for this prediction\n",
    "            result = {\n",
    "                \"item_id\": i + 1,\n",
    "                \"premise\": premise,\n",
    "                \"hypothesis\": hypothesis,\n",
    "                \"true_label_raw\": true_label_raw,\n",
    "                \"true_label_normalized\": true_label_normalized,\n",
    "                \"predicted_label\": prediction_data[\"predicted_label\"],\n",
    "                \"raw_output\": prediction_data[\"raw_output\"],\n",
    "                \"inference_time\": prediction_data[\"inference_time\"],\n",
    "                \"is_valid\": prediction_data[\"is_valid\"]\n",
    "            }\n",
    "            model_results.append(result)\n",
    "\n",
    "            # --- 4. Evaluate Prediction ---\n",
    "            if prediction_data[\"is_valid\"]:\n",
    "                # If the model produced a valid label, count it\n",
    "                total_valid += 1\n",
    "                \n",
    "                # Normalize the predicted label (should already be standard, but for safety)\n",
    "                pred_label_normalized = LABEL_MAPPING.get(prediction_data[\"predicted_label\"], prediction_data[\"predicted_label\"])\n",
    "                \n",
    "                # Check if the prediction is correct\n",
    "                if pred_label_normalized == true_label_normalized:\n",
    "                    correct_predictions += 1\n",
    "                    is_prediction_correct = True\n",
    "                else:\n",
    "                    is_prediction_correct = False\n",
    "                    \n",
    "                print(f\"  -> Predicted: '{prediction_data['predicted_label']}' (True: {true_label_raw}) - {'Correct' if is_prediction_correct else 'Incorrect'}\")\n",
    "                \n",
    "                # Add to total inference time for valid predictions\n",
    "                total_inference_time += prediction_data[\"inference_time\"]\n",
    "            else:\n",
    "                # If the model failed to produce a parseable label, note it as an invalid prediction\n",
    "                print(f\"  -> Invalid prediction: '{prediction_data['raw_output']}' (True: {true_label_raw})\")\n",
    "\n",
    "            # Small delay to prevent potential issues with rapid API calls\n",
    "            time.sleep(0.1) \n",
    "\n",
    "        # --- 5. Calculate Metrics ---\n",
    "        # Accuracy is defined as the number of correct predictions divided by the number of valid predictions.\n",
    "        # This aligns with the assignment's definition: \"Accuracy = Correct Valid Predictions / Total Valid Predictions\"\n",
    "        accuracy = correct_predictions / total_valid if total_valid > 0 else 0\n",
    "        \n",
    "        # Reject Rate is defined as the number of invalid predictions divided by the total number of items.\n",
    "        # This aligns with the assignment's definition: \"Reject Rate = Invalid Predictions / Total Items\"\n",
    "        reject_rate = (len(model_results) - total_valid) / len(model_results) if len(model_results) > 0 else 0\n",
    "        \n",
    "        # Calculate average inference time for valid predictions\n",
    "        average_inference_time = total_inference_time / total_valid if total_valid > 0 else 0\n",
    "        average_inference_time = round(average_inference_time, 2)\n",
    "        \n",
    "        # --- 6. Store Final Results for this Model ---\n",
    "        final_result = {\n",
    "            \"model\": model,\n",
    "            \"accuracy\": round(accuracy, 4),\n",
    "            \"reject_rate\": round(reject_rate, 4),\n",
    "            \"average_inference_time\": average_inference_time,  # Added as per assignment requirement\n",
    "            \"total_items\": len(model_results),\n",
    "            \"valid_predictions\": total_valid,\n",
    "            \"correct_predictions\": correct_predictions,\n",
    "            \"predictions\": model_results # Includes detailed info for each of the 100 items\n",
    "        }\n",
    "        \n",
    "        all_results.append(final_result)\n",
    "        \n",
    "        # --- 7. Print Summary ---\n",
    "        print(f\"\\n--- Summary for Model: {model} (Full Dataset, Standard Approach) ---\")\n",
    "        print(f\"Accuracy (on valid predictions): {accuracy:.4f}\")\n",
    "        print(f\"Reject Rate: {reject_rate:.4f}\")\n",
    "        print(f\"Average Inference Time: {average_inference_time:.2f} seconds\")\n",
    "        print(f\"Valid Predictions: {total_valid}/{len(model_results)}\")\n",
    "\n",
    "    # --- 8. Save Results to File ---\n",
    "    os.makedirs(\"json_outputs\", exist_ok=True)\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n[Success] NLI predictions and metrics saved to '{output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to save results to '{output_file}': {e}\")\n",
    "\n",
    "\n",
    "# --- Entry Point ---\n",
    "# This section executes when the script is run directly.\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a random seed for reproducibility, as required by the assignment.\n",
    "    # Using the same seed, model, and prompt should yield identical outputs.\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Run the main NLI classification function on the full dataset.\n",
    "    run_nli_classification_standard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264f052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Loaded 100 items from the dataset.\n",
      "\n",
      "======================================================================\n",
      "[+] Running NLI Classification using model: qwen2.5vl:3b on FULL dataset (100 examples)\n",
      "======================================================================\n",
      "\n",
      "[001/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[002/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: neutral) - Incorrect\n",
      "\n",
      "[003/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[004/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[005/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[006/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[007/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[008/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[009/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[010/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[011/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[012/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[013/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[014/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[015/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[016/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[017/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[018/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[019/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[020/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[021/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[022/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[023/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[024/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[025/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[026/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[027/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: entails) - Incorrect\n",
      "\n",
      "[028/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[029/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[030/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: neutral) - Incorrect\n",
      "\n",
      "[031/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[032/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[033/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[034/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[035/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[036/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[037/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[038/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[039/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[040/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[041/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[042/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[043/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[044/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[045/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[046/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[047/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[048/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[049/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[050/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[051/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[052/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[053/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[054/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[055/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[056/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[057/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[058/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[059/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[060/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[061/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[062/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[063/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[064/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[065/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[066/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[067/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[068/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[069/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[070/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[071/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[072/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[073/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[074/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[075/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[076/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[077/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[078/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[079/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[080/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[081/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[082/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[083/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[084/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[085/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[086/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[087/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[088/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[089/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[090/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[091/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[092/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: contradicts) - Incorrect\n",
      "\n",
      "[093/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[094/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[095/100] Classifying...\n",
      "  -> Predicted: 'neutral' (True: neutral) - Correct\n",
      "\n",
      "[096/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[097/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: entails) - Correct\n",
      "\n",
      "[098/100] Classifying...\n",
      "  -> Predicted: 'entailment' (True: neutral) - Incorrect\n",
      "\n",
      "[099/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "[100/100] Classifying...\n",
      "  -> Predicted: 'contradiction' (True: contradicts) - Correct\n",
      "\n",
      "--- Summary for Model: qwen2.5vl:3b (Full Dataset, Standard Approach) ---\n",
      "Accuracy (on valid predictions): 0.8300\n",
      "Reject Rate: 0.0000\n",
      "Average Inference Time: 5.78 seconds\n",
      "Valid Predictions: 100/100\n",
      "\n",
      "[Success] NLI predictions and metrics saved to 'json_outputs/nli_output_qwen.json'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Natural Language Inference (NLI) - NLP Assignment 4, Section 3.3\n",
    "Running Classification using Qwen2.5vl:3b\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ollama API endpoint\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Model to be used for NLI task\n",
    "# As per assignment instructions, this is one of the two required models.\n",
    "# Qwen2.5vl:3b was selected as the second model for comparison.\n",
    "MODELS = [\"qwen2.5vl:3b\"]\n",
    "\n",
    "# Standard labels expected from the LLM for the NLI task\n",
    "MODEL_NLI_LABELS = [\"entailment\", \"contradiction\", \"neutral\"]\n",
    "\n",
    "# Mapping to normalize labels from the dataset to match model outputs.\n",
    "# The dataset might use 'entails'/'contradicts', while the model outputs 'entailment'/'contradiction'.\n",
    "# This ensures accurate calculation of accuracy.\n",
    "LABEL_MAPPING = {\n",
    "    \"entails\": \"entailment\",\n",
    "    \"entailment\": \"entailment\",\n",
    "    \"contradicts\": \"contradiction\",\n",
    "    \"contradiction\": \"contradiction\",\n",
    "    \"neutral\": \"neutral\"\n",
    "}\n",
    "\n",
    "# Prompt template for the NLI task.\n",
    "# Provides a clear definition of the task and expects a single-word answer.\n",
    "STANDARD_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert at NLI (Natural Language Inference). Given a premise and a hypothesis, determine if the relationship is:\n",
    "- entailment (the hypothesis must be true if the premise is true)\n",
    "- contradiction (the hypothesis must be false if the premise is true)\n",
    "- neutral (the hypothesis is neither entailed nor contradicted by the premise)\n",
    "\n",
    "Answer ONLY with one of these three words: entailment, contradiction, neutral\n",
    "\n",
    "Premise: {premise}\n",
    "Hypothesis: {hypothesis}\n",
    "Relationship:\n",
    "\"\"\".strip()\n",
    "\n",
    "# Decoding parameters to ensure consistent and concise outputs for classification.\n",
    "# Low temperature for determinism, stop tokens to prevent extra text.\n",
    "STANDARD_DECODING_PARAMS = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 15,\n",
    "    \"stop\": [\"\\n\", \".\", \" \"]\n",
    "}\n",
    "\n",
    "def classify_nli_standard(model_name, premise, hypothesis):\n",
    "    \"\"\"\n",
    "    Classifies the relationship between a premise and hypothesis using an LLM via Ollama API.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the LLM to use (e.g., \"qwen2.5vl:3b\").\n",
    "        premise (str): The premise sentence.\n",
    "        hypothesis (str): The hypothesis sentence.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the predicted label, raw model output,\n",
    "              inference time, and validity status.\n",
    "              - predicted_label (str): The extracted NLI label (entailment, contradiction, neutral) or raw output if invalid.\n",
    "              - raw_output (str): The complete, unprocessed text returned by the model.\n",
    "              - inference_time (float): The time taken for the model to respond, in seconds.\n",
    "              - is_valid (bool): True if a valid label was successfully extracted, False otherwise.\n",
    "    \"\"\"\n",
    "    # Format the prompt with the current premise and hypothesis\n",
    "    prompt = STANDARD_PROMPT_TEMPLATE.format(premise=premise, hypothesis=hypothesis)\n",
    "\n",
    "    # Prepare the payload for the Ollama API request\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": STANDARD_DECODING_PARAMS\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Send the request to the Ollama API\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        inference_time = round(time.time() - start_time, 2)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            raw_output = response.json()['response'].strip().lower()\n",
    "            \n",
    "            # --- Parsing Logic ---\n",
    "            # 1. Look for the label at the very beginning of the output (most reliable)\n",
    "            predicted_label = None\n",
    "            for label in MODEL_NLI_LABELS:\n",
    "                if raw_output.startswith(label):\n",
    "                    predicted_label = label\n",
    "                    break\n",
    "\n",
    "            # 2. If not found at the start, check if the label appears anywhere in the output (fallback)\n",
    "            if not predicted_label:\n",
    "                 for label in MODEL_NLI_LABELS:\n",
    "                     if label in raw_output:\n",
    "                         predicted_label = label\n",
    "                         break\n",
    "            \n",
    "            # 3. Determine if the prediction is valid (i.e., a label was found)\n",
    "            is_valid = predicted_label is not None\n",
    "            \n",
    "            # 4. If no valid label was found, store the raw output for debugging\n",
    "            if not is_valid:\n",
    "                predicted_label = raw_output \n",
    "\n",
    "            return {\n",
    "                \"predicted_label\": predicted_label,\n",
    "                \"raw_output\": raw_output,\n",
    "                \"inference_time\": inference_time,\n",
    "                \"is_valid\": is_valid\n",
    "            }\n",
    "        else:\n",
    "            # Handle unsuccessful API request\n",
    "            return {\n",
    "                \"predicted_label\": None,\n",
    "                \"raw_output\": f\"[Error] Status {response.status_code}\",\n",
    "                \"inference_time\": inference_time,\n",
    "                \"is_valid\": False\n",
    "            }\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions during the API call\n",
    "        return {\n",
    "            \"predicted_label\": None,\n",
    "            \"raw_output\": f\"[Exception] {e}\",\n",
    "            \"inference_time\": round(time.time() - start_time, 2),\n",
    "            \"is_valid\": False\n",
    "        }\n",
    "\n",
    "\n",
    "def run_nli_classification_standard(input_file=\"datasets/nli/nli.csv\", output_file=\"json_outputs/nli_output_qwen.json\"):\n",
    "    \"\"\"\n",
    "    Main function to load the full NLI dataset, run classification for the specified model,\n",
    "    calculate performance metrics, and save the detailed results.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file containing the NLI data (premise, hypothesis, label).\n",
    "                          As per the assignment PDF, this truncated dataset contains exactly 100 items.\n",
    "        output_file (str): Path to the output JSON file where results and metrics will be saved.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        # Load the NLI data from the provided CSV file\n",
    "        df = pd.read_csv(input_file)\n",
    "        # Convert the DataFrame to a list of dictionaries for easy iteration\n",
    "        nli_data = df.to_dict('records')\n",
    "        print(f\"[Info] Loaded {len(nli_data)} items from the dataset.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[Error] File '{input_file}' not found. Please check the file path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read the CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # This will store the final results for all models (though we are only using one here)\n",
    "    all_results = []\n",
    "\n",
    "    # --- 2. Run Classification for Each Model ---\n",
    "    for model in MODELS:\n",
    "        print(f\"\\n{'='*70}\\n[+] Running NLI Classification using model: {model} on FULL dataset ({len(nli_data)} examples)\\n{'='*70}\")\n",
    "        \n",
    "        # Store results and counters for this specific model\n",
    "        model_results = []\n",
    "        correct_predictions = 0\n",
    "        total_valid = 0\n",
    "        total_inference_time = 0.0  # For calculating average inference time\n",
    "        \n",
    "        # --- 3. Iterate Through Dataset ---\n",
    "        for i, item in enumerate(nli_data):\n",
    "            # Extract data for the current item\n",
    "            premise = item[\"premise\"]\n",
    "            hypothesis = item[\"hypothesis\"]\n",
    "            true_label_raw = item[\"label\"] \n",
    "            \n",
    "            # Normalize the true label from the dataset to match our standard labels\n",
    "            true_label_normalized = LABEL_MAPPING.get(true_label_raw, true_label_raw)\n",
    "\n",
    "            print(f\"\\n[{i+1:03d}/{len(nli_data)}] Classifying...\")\n",
    "\n",
    "            # Get the model's prediction\n",
    "            prediction_data = classify_nli_standard(model, premise, hypothesis)\n",
    "            \n",
    "            # Store detailed information for this prediction\n",
    "            result = {\n",
    "                \"item_id\": i + 1,\n",
    "                \"premise\": premise,\n",
    "                \"hypothesis\": hypothesis,\n",
    "                \"true_label_raw\": true_label_raw,\n",
    "                \"true_label_normalized\": true_label_normalized,\n",
    "                \"predicted_label\": prediction_data[\"predicted_label\"],\n",
    "                \"raw_output\": prediction_data[\"raw_output\"],\n",
    "                \"inference_time\": prediction_data[\"inference_time\"],\n",
    "                \"is_valid\": prediction_data[\"is_valid\"]\n",
    "            }\n",
    "            model_results.append(result)\n",
    "\n",
    "            # --- 4. Evaluate Prediction ---\n",
    "            if prediction_data[\"is_valid\"]:\n",
    "                # If the model produced a valid label, count it\n",
    "                total_valid += 1\n",
    "                \n",
    "                # Normalize the predicted label (should already be standard, but for safety)\n",
    "                pred_label_normalized = LABEL_MAPPING.get(prediction_data[\"predicted_label\"], prediction_data[\"predicted_label\"])\n",
    "                \n",
    "                # Check if the prediction is correct\n",
    "                if pred_label_normalized == true_label_normalized:\n",
    "                    correct_predictions += 1\n",
    "                    is_prediction_correct = True\n",
    "                else:\n",
    "                    is_prediction_correct = False\n",
    "                    \n",
    "                print(f\"  -> Predicted: '{prediction_data['predicted_label']}' (True: {true_label_raw}) - {'Correct' if is_prediction_correct else 'Incorrect'}\")\n",
    "                \n",
    "                # Add to total inference time for valid predictions\n",
    "                total_inference_time += prediction_data[\"inference_time\"]\n",
    "            else:\n",
    "                # If the model failed to produce a parseable label, note it as an invalid prediction\n",
    "                print(f\"  -> Invalid prediction: '{prediction_data['raw_output']}' (True: {true_label_raw})\")\n",
    "\n",
    "            # Small delay to prevent potential issues with rapid API calls\n",
    "            time.sleep(0.1) \n",
    "\n",
    "        # --- 5. Calculate Metrics ---\n",
    "        # Accuracy is defined as the number of correct predictions divided by the number of valid predictions.\n",
    "        # This aligns with the assignment's definition: \"Accuracy = Correct Valid Predictions / Total Valid Predictions\"\n",
    "        accuracy = correct_predictions / total_valid if total_valid > 0 else 0\n",
    "        \n",
    "        # Reject Rate is defined as the number of invalid predictions divided by the total number of items.\n",
    "        # This aligns with the assignment's definition: \"Reject Rate = Invalid Predictions / Total Items\"\n",
    "        reject_rate = (len(model_results) - total_valid) / len(model_results) if len(model_results) > 0 else 0\n",
    "        \n",
    "        # Calculate average inference time for valid predictions\n",
    "        average_inference_time = total_inference_time / total_valid if total_valid > 0 else 0\n",
    "        average_inference_time = round(average_inference_time, 2)\n",
    "        \n",
    "        # --- 6. Store Final Results for this Model ---\n",
    "        final_result = {\n",
    "            \"model\": model,\n",
    "            \"accuracy\": round(accuracy, 4),\n",
    "            \"reject_rate\": round(reject_rate, 4),\n",
    "            \"average_inference_time\": average_inference_time,  # Added as per assignment requirement\n",
    "            \"total_items\": len(model_results),\n",
    "            \"valid_predictions\": total_valid,\n",
    "            \"correct_predictions\": correct_predictions,\n",
    "            \"predictions\": model_results # Includes detailed info for each of the 100 items\n",
    "        }\n",
    "        \n",
    "        all_results.append(final_result)\n",
    "        \n",
    "        # --- 7. Print Summary ---\n",
    "        print(f\"\\n--- Summary for Model: {model} (Full Dataset, Standard Approach) ---\")\n",
    "        print(f\"Accuracy (on valid predictions): {accuracy:.4f}\")\n",
    "        print(f\"Reject Rate: {reject_rate:.4f}\")\n",
    "        print(f\"Average Inference Time: {average_inference_time:.2f} seconds\")\n",
    "        print(f\"Valid Predictions: {total_valid}/{len(model_results)}\")\n",
    "\n",
    "    # --- 8. Save Results to File ---\n",
    "    os.makedirs(\"json_outputs\", exist_ok=True)\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n[Success] NLI predictions and metrics saved to '{output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to save results to '{output_file}': {e}\")\n",
    "\n",
    "\n",
    "# --- Entry Point ---\n",
    "# This section executes when the script is run directly.\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a random seed for reproducibility, as required by the assignment.\n",
    "    # Using the same seed, model, and prompt should yield identical outputs.\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Run the main NLI classification function on the full dataset.\n",
    "    run_nli_classification_standard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5353f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Image Captioning (IC), Section 3.4\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# Ollama API endpoint\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Vision-capable model as required\n",
    "MODELS = [\"qwen2.5vl:3b\"]\n",
    "\n",
    "# Caption generation prompts\n",
    "IC_PROMPT_TEMPLATES = [\n",
    "    \"Describe the image in a single sentence. Image Description:\",\n",
    "    \"Write a concise caption for the image. Image Description:\",\n",
    "    \"Generate a natural language description for the given image. Image Description:\"\n",
    "]\n",
    "\n",
    "# Generation parameters\n",
    "IC_DECODING_PARAMS = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Convert image to base64 string for API.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def generate_caption(model_name, image_path):\n",
    "    \"\"\"Generate caption for image using Ollama API.\"\"\"\n",
    "    prompt = random.choice(IC_PROMPT_TEMPLATES)\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"images\": [image_base64],\n",
    "        \"options\": {\n",
    "            \"temperature\": IC_DECODING_PARAMS[\"temperature\"],\n",
    "            \"top_p\": IC_DECODING_PARAMS[\"top_p\"],\n",
    "            \"max_tokens\": 50,\n",
    "            \"stop\": [\"\\n\", \".\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        inference_time = round(time.time() - start_time, 2)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            caption = response.json()['response'].strip()\n",
    "            is_valid = len(caption.split()) > 5 and not any(\n",
    "                prompt in caption for prompt in [\"Here is a caption\", \"Caption:\", \"Image Description:\"]\n",
    "            )\n",
    "            return {\"caption\": caption, \"inference_time\": inference_time, \"is_valid\": is_valid}\n",
    "        return {\"caption\": None, \"inference_time\": inference_time, \"is_valid\": False}\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"caption\": f\"[Error] {e}\",\n",
    "            \"inference_time\": round(time.time() - start_time, 2),\n",
    "            \"is_valid\": False\n",
    "        }\n",
    "\n",
    "def calculate_cider_scores(generated_captions, reference_captions):\n",
    "    \"\"\"Calculate CIDEr scores against reference captions.\"\"\"\n",
    "    res = [{\"image_id\": img_id, \"caption\": caption} for img_id, caption in generated_captions.items()]\n",
    "    gts = {img_id: captions for img_id, captions in reference_captions.items()}\n",
    "    \n",
    "    cider_score, cider_scores = Cider().compute_score(gts, res)\n",
    "    \n",
    "    cider_details = {img_id: float(score) for img_id, score in zip(generated_captions.keys(), cider_scores)}\n",
    "    return float(cider_score), cider_details\n",
    "\n",
    "def run_image_captioning(input_file=\"datasets/ic/ic.csv\", images_dir=\"datasets/ic/images\", output_file=\"json_outputs/ic_output.json\"):\n",
    "    \"\"\"Process images, generate captions, and evaluate with CIDEr.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"[Info] Loaded {len(df)} images from dataset.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[Error] File '{input_file}' not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    generated_captions = {}\n",
    "    reference_captions = {}\n",
    "\n",
    "    for model in MODELS:\n",
    "        print(f\"\\n{'='*60}\\n[+] Running Image Captioning with {model}\\n{'='*60}\")\n",
    "        \n",
    "        model_results = []\n",
    "        total_valid = 0\n",
    "        total_inference_time = 0.0\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            # Use the 'image' column as the image ID \n",
    "            image_id = str(row[\"image\"])\n",
    "            image_path = os.path.join(images_dir, image_id)  \n",
    "            \n",
    "            # Check if the image file actually exists\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"[Error] Image file not found: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Store 5 reference captions per image from the 'human_captions' column\n",
    "            try:\n",
    "                ref_captions = eval(row[\"human_captions\"])\n",
    "                if not isinstance(ref_captions, list) or len(ref_captions) != 5:\n",
    "                    raise ValueError(\"Not a valid list of 5 captions\")\n",
    "            except:\n",
    "                # Fallback in case the parsing fails\n",
    "                ref_captions = [\"\"] * 5\n",
    "            \n",
    "            reference_captions[image_id] = ref_captions\n",
    "            \n",
    "            print(f\"\\n[{i+1:03d}] Processing image: {image_path}\")\n",
    "            \n",
    "            caption_data = generate_caption(model, image_path)\n",
    "            \n",
    "            result = {\n",
    "                \"model\": model,\n",
    "                \"image_id\": image_id,\n",
    "                \"image_path\": image_path,\n",
    "                \"caption\": caption_data[\"caption\"],\n",
    "                \"inference_time\": caption_data[\"inference_time\"],\n",
    "                \"is_valid\": caption_data[\"is_valid\"]\n",
    "            }\n",
    "            model_results.append(result)\n",
    "            \n",
    "            if caption_data[\"is_valid\"]:\n",
    "                total_valid += 1\n",
    "                total_inference_time += caption_data[\"inference_time\"]\n",
    "                generated_captions[image_id] = caption_data[\"caption\"]\n",
    "                print(\" Caption generated successfully.\")\n",
    "            else:\n",
    "                print(\" Invalid caption.\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        reject_rate = (len(model_results) - total_valid) / len(model_results) if len(model_results) > 0 else 0\n",
    "        reject_rate = round(reject_rate, 4)\n",
    "        \n",
    "        average_inference_time = total_inference_time / total_valid if total_valid > 0 else 0\n",
    "        average_inference_time = round(average_inference_time, 2)\n",
    "        \n",
    "        cider_score = 0.0\n",
    "        cider_details = {}\n",
    "        if total_valid > 0:\n",
    "            try:\n",
    "                cider_score, cider_details = calculate_cider_scores(generated_captions, reference_captions)\n",
    "                cider_score = round(cider_score, 4)\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] CIDEr calculation failed: {e}\")\n",
    "        \n",
    "        # Store results\n",
    "        final_result = {\n",
    "            \"model\": model,\n",
    "            \"total_images\": len(df),\n",
    "            \"valid_captions\": total_valid,\n",
    "            \"reject_rate\": reject_rate,\n",
    "            \"average_inference_time\": average_inference_time,\n",
    "            \"cider_score\": cider_score,\n",
    "            \"cider_details\": cider_details,\n",
    "            \"predictions\": model_results\n",
    "        }\n",
    "        results.append(final_result)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n--- Results for {model} ---\")\n",
    "        print(f\"Total Images: {len(df)}\")\n",
    "        print(f\"Valid Captions: {total_valid}/{len(df)}\")\n",
    "        print(f\"Reject Rate: {reject_rate:.4f}\")\n",
    "        print(f\"Average Inference Time: {average_inference_time:.2f} seconds\")\n",
    "        print(f\"CIDEr Score: {cider_score:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(\"json_outputs\", exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n[Success] Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)\n",
    "    \n",
    "    run_image_captioning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31be53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] CIDEr evaluation libraries installed.\n",
      "[Info] Found 10 JSON files to merge.\n",
      "\n",
      "[Info] Calculating overall CIDEr score for the entire dataset...\n",
      "\n",
      "============================================================\n",
      "[+] Overall Results for Image Captioning (All 100 Images)\n",
      "============================================================\n",
      "Total Images: 100\n",
      "Valid Captions: 100/100\n",
      "Reject Rate: 0.0000\n",
      "Average Inference Time: 146.74 seconds\n",
      "CIDEr Score: 0.6318\n",
      "\n",
      "[Success] Overall results saved to json_outputs/ic_output.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge JSON output files from Image Captioning subsets and calculate overall metrics\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def merge_json_files(json_files):\n",
    "    \"\"\"Merge multiple JSON files into a single structure with all predictions.\"\"\"\n",
    "    all_predictions = []\n",
    "    generated_captions = {}\n",
    "    reference_captions = {}\n",
    "    \n",
    "    total_images = 0\n",
    "    total_valid = 0\n",
    "    total_inference_time = 0.0\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Process each model result in the file (usually just one model)\n",
    "            for model_result in data:\n",
    "                total_images += model_result[\"total_images\"]\n",
    "                total_valid += model_result[\"valid_captions\"]\n",
    "                total_inference_time += model_result[\"valid_captions\"] * model_result[\"average_inference_time\"]\n",
    "                \n",
    "                # Collect all predictions\n",
    "                for prediction in model_result[\"predictions\"]:\n",
    "                    all_predictions.append(prediction)\n",
    "                    \n",
    "                    if prediction[\"is_valid\"]:\n",
    "                        generated_captions[prediction[\"image_id\"]] = prediction[\"caption\"]\n",
    "                    \n",
    "                    # Extract reference captions from the prediction structure\n",
    "                    # This assumes the reference captions are stored in a consistent way\n",
    "                    # You might need to adjust this part based on your actual data structure\n",
    "                    if \"reference_captions\" in prediction:  # If directly stored\n",
    "                        reference_captions[prediction[\"image_id\"]] = prediction[\"reference_captions\"]\n",
    "                    else:\n",
    "                        # Try to extract from the human_captions column if available\n",
    "                        try:\n",
    "                            # This is a simplified approach - you might need to adjust based on your actual data\n",
    "                            ref_captions = eval(prediction[\"image_id\"].split('.')[0] + \"_ref_captions\")\n",
    "                            if isinstance(ref_captions, list) and len(ref_captions) == 5:\n",
    "                                reference_captions[prediction[\"image_id\"]] = ref_captions\n",
    "                        except:\n",
    "                            pass\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    reject_rate = (total_images - total_valid) / total_images if total_images > 0 else 0\n",
    "    average_inference_time = total_inference_time / total_valid if total_valid > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"all_predictions\": all_predictions,\n",
    "        \"generated_captions\": generated_captions,\n",
    "        \"reference_captions\": reference_captions,\n",
    "        \"total_images\": total_images,\n",
    "        \"total_valid\": total_valid,\n",
    "        \"reject_rate\": reject_rate,\n",
    "        \"average_inference_time\": average_inference_time\n",
    "    }\n",
    "\n",
    "def calculate_overall_cider(generated_captions, reference_captions):\n",
    "    \"\"\"Calculate CIDEr score for the entire dataset.\"\"\"\n",
    "    # Ensure we're working with dictionaries\n",
    "    if not isinstance(generated_captions, dict) or not isinstance(reference_captions, dict):\n",
    "        print(\"[Error] Invalid data format for CIDEr calculation\")\n",
    "        return 0.0, {}\n",
    "    \n",
    "    # CRITICAL CHANGE: Proper formatting for pycocoevalcap\n",
    "    # In COCO evaluation format:\n",
    "    # - res should be {img_id: [caption1, caption2, ...]} but we have only one caption per image\n",
    "    # - gts should be {img_id: [ref_caption1, ref_caption2, ..., ref_caption5]}\n",
    "    res = {str(img_id): [caption] for img_id, caption in generated_captions.items()}\n",
    "    gts = {str(img_id): captions for img_id, captions in reference_captions.items()}\n",
    "    \n",
    "    # Ensure all image IDs are the same in both structures\n",
    "    common_ids = set(gts.keys()) & set(res.keys())\n",
    "    \n",
    "    if not common_ids:\n",
    "        print(\"[Error] No common image IDs found between generated and reference captions\")\n",
    "        return 0.0, {}\n",
    "    \n",
    "    # Filter to keep only common IDs\n",
    "    filtered_res = {img_id: res[img_id] for img_id in common_ids}\n",
    "    filtered_gts = {img_id: gts[img_id] for img_id in common_ids}\n",
    "    \n",
    "    try:\n",
    "        # Calculate CIDEr score\n",
    "        cider = Cider()\n",
    "        cider_score, cider_scores = cider.compute_score(filtered_gts, filtered_res)\n",
    "        \n",
    "        # Handle different return types of cider_scores\n",
    "        cider_details = {}\n",
    "        if isinstance(cider_scores, list):\n",
    "            # If it's a list, map it to image IDs\n",
    "            for i, img_id in enumerate(common_ids):\n",
    "                cider_details[img_id] = float(cider_scores[i])\n",
    "        elif isinstance(cider_scores, dict):\n",
    "            # If it's a dict, use it directly\n",
    "            cider_details = {img_id: float(score) for img_id, score in cider_scores.items()}\n",
    "        elif isinstance(cider_scores, float):\n",
    "            # If it's a single float, apply to all images\n",
    "            cider_details = {img_id: cider_scores for img_id in common_ids}\n",
    "            cider_score = cider_scores\n",
    "        else:\n",
    "            # Fallback: assign the same score to all images\n",
    "            cider_details = {img_id: float(cider_score) for img_id in common_ids}\n",
    "            \n",
    "        return float(cider_score), cider_details\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] CIDEr calculation failed: {str(e)}\")\n",
    "        # Return default values in case of error\n",
    "        return 0.0, {img_id: 0.0 for img_id in common_ids}\n",
    "\n",
    "def load_reference_captions_from_csv(csv_file=\"datasets/ic/ic.csv\"):\n",
    "    \"\"\"Load reference captions from the original CSV file.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        reference_captions = {}\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            image_id = str(row[\"image\"])\n",
    "            try:\n",
    "                # Safely evaluate the human_captions string\n",
    "                ref_captions = eval(row[\"human_captions\"])\n",
    "                if isinstance(ref_captions, list) and len(ref_captions) == 5:\n",
    "                    reference_captions[image_id] = ref_captions\n",
    "            except:\n",
    "                # Fallback in case the parsing fails\n",
    "                reference_captions[image_id] = [\"\"] * 5\n",
    "                \n",
    "        return reference_captions\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load reference captions from CSV: {e}\")\n",
    "        return {}\n",
    "\n",
    "def main():\n",
    "    # List of JSON output files to merge\n",
    "    json_files = [\n",
    "        \"json_outputs/ic_output_subset_10.json\",\n",
    "        \"json_outputs/ic_output_subset_11_20.json\",\n",
    "        \"json_outputs/ic_output_subset_21_30.json\",\n",
    "        \"json_outputs/ic_output_subset_31_40.json\",\n",
    "        \"json_outputs/ic_output_subset_41_50.json\",\n",
    "        \"json_outputs/ic_output_subset_51_60.json\",\n",
    "        \"json_outputs/ic_output_subset_61_70.json\",\n",
    "        \"json_outputs/ic_output_subset_71_80.json\",\n",
    "        \"json_outputs/ic_output_subset_81_90.json\",\n",
    "        \"json_outputs/ic_output_subset_91_100.json\"\n",
    "    ]\n",
    "    \n",
    "    # Filter out files that don't exist\n",
    "    existing_files = [f for f in json_files if os.path.exists(f)]\n",
    "    if not existing_files:\n",
    "        print(\"[Error] No JSON files found. Please check the file paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"[Info] Found {len(existing_files)} JSON files to merge.\")\n",
    "    \n",
    "    # Step 1: Merge the JSON files and calculate basic metrics\n",
    "    merged_data = merge_json_files(existing_files)\n",
    "    \n",
    "    # Step 2: Load reference captions from CSV (more reliable than from JSON files)\n",
    "    reference_captions = load_reference_captions_from_csv()\n",
    "    \n",
    "    # Step 3: Calculate overall CIDEr score\n",
    "    print(\"\\n[Info] Calculating overall CIDEr score for the entire dataset...\")\n",
    "    cider_score, cider_details = calculate_overall_cider(\n",
    "        merged_data[\"generated_captions\"], \n",
    "        reference_captions\n",
    "    )\n",
    "    cider_score = round(cider_score, 4)\n",
    "    \n",
    "    # Step 4: Create final results structure\n",
    "    final_result = {\n",
    "        \"model\": \"qwen2.5vl:3b\",  # Assuming all subsets used the same model\n",
    "        \"total_images\": merged_data[\"total_images\"],\n",
    "        \"valid_captions\": merged_data[\"total_valid\"],\n",
    "        \"reject_rate\": round(merged_data[\"reject_rate\"], 4),\n",
    "        \"average_inference_time\": round(merged_data[\"average_inference_time\"], 2),\n",
    "        \"cider_score\": cider_score,\n",
    "        \"cider_details\": cider_details,\n",
    "        \"predictions\": merged_data[\"all_predictions\"]\n",
    "    }\n",
    "    \n",
    "    # Step 5: Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[+] Overall Results for Image Captioning (All 100 Images)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Images: {final_result['total_images']}\")\n",
    "    print(f\"Valid Captions: {final_result['valid_captions']}/{final_result['total_images']}\")\n",
    "    print(f\"Reject Rate: {final_result['reject_rate']:.4f}\")\n",
    "    print(f\"Average Inference Time: {final_result['average_inference_time']:.2f} seconds\")\n",
    "    print(f\"CIDEr Score: {final_result['cider_score']:.4f}\")\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    output_file = \"json_outputs/ic_output.json\"\n",
    "    os.makedirs(\"json_outputs\", exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([final_result], f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n[Success] Overall results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        from pycocoevalcap.cider.cider import Cider\n",
    "        print(\"[Info] CIDEr evaluation libraries installed.\")\n",
    "    except ImportError:\n",
    "        print(\"[Error] CIDEr libraries not installed.\")\n",
    "        print(\"Install with: pip install pycocoevalcap==1.2\")\n",
    "        exit(1)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64417d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visual Question Answering (VQA), Section 3.5\n",
    "Full Dataset Evaluation\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "import re\n",
    "\n",
    "# Ollama API endpoint\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Vision-capable model as required\n",
    "MODELS = [\"qwen2.5vl:3b\"]\n",
    "\n",
    "# Simple prompt template for VQA\n",
    "VQA_PROMPT_TEMPLATE = \"\"\"\n",
    "Look at the image and answer the question by selecting ONLY ONE LETTER from A, B, C, D, or E.\n",
    "\n",
    "Question: {question}\n",
    "Options: {options}\n",
    "\n",
    "Think carefully and then write ONLY the letter of your answer (A, B, C, D, or E):\n",
    "\"\"\".strip()\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Convert image to base64 string for API.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def extract_answer(response_text):\n",
    "    \"\"\"Extract the answer letter (A-E) from model's response.\"\"\"\n",
    "    # Normalize response text\n",
    "    response_text = response_text.strip().upper()\n",
    "    \n",
    "    # Strategy 1: Look for the answer at the very end\n",
    "    last_chars = response_text[-10:] if len(response_text) > 10 else response_text\n",
    "    for char in reversed(last_chars):\n",
    "        if char in ['A', 'B', 'C', 'D', 'E']:\n",
    "            return char\n",
    "    \n",
    "    # Strategy 2: Look for the answer at the very beginning\n",
    "    first_chars = response_text[:10] if len(response_text) > 10 else response_text\n",
    "    for char in first_chars:\n",
    "        if char in ['A', 'B', 'C', 'D', 'E']:\n",
    "            return char\n",
    "            \n",
    "    # Strategy 3: Look for patterns like \"Answer: C\" or \"The answer is C\"\n",
    "    patterns = [\n",
    "        r'ANSWER\\s*[:=]?\\s*([A-E])',\n",
    "        r'THE\\s+ANSWER\\s+IS\\s+([A-E])',\n",
    "        r'OPTION\\s+([A-E])',\n",
    "        r'\\b([A-E])\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response_text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    # If no valid answer found\n",
    "    return None\n",
    "\n",
    "def generate_vqa_answer(model_name, image_path, question, options):\n",
    "    \"\"\"Generate VQA answer using Ollama API.\"\"\"\n",
    "    prompt = VQA_PROMPT_TEMPLATE.format(question=question, options=options)\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"images\": [image_base64],\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0,  # Deterministic\n",
    "            \"top_p\": 0.95,\n",
    "            \"max_tokens\": 10,\n",
    "            \"stop\": [\"\\n\", \".\", \" \"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_URL, json=payload)\n",
    "        inference_time = round(time.time() - start_time, 2)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            raw_output = response.json()['response'].strip()\n",
    "            answer = extract_answer(raw_output)\n",
    "            is_valid = answer is not None\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"raw_output\": raw_output,\n",
    "                \"inference_time\": inference_time,\n",
    "                \"is_valid\": is_valid\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"answer\": None,\n",
    "                \"raw_output\": f\"[Error] Status {response.status_code}\",\n",
    "                \"inference_time\": inference_time,\n",
    "                \"is_valid\": False\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": None,\n",
    "            \"raw_output\": f\"[Error] {e}\",\n",
    "            \"inference_time\": round(time.time() - start_time, 2),\n",
    "            \"is_valid\": False\n",
    "        }\n",
    "\n",
    "def run_vqa_full(input_file=\"datasets/vqa/vqa.csv\", images_dir=\"datasets/vqa/images\", output_file=\"json_outputs/vqa_output_full.json\"):\n",
    "    \"\"\"Process all questions from the VQA dataset.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"[Info] Loaded {len(df)} questions from VQA dataset.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[Error] File '{input_file}' not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # Process ALL questions in the dataset\n",
    "    df_full = df.copy()\n",
    "    # Reset index to make question_id start from 1 for this subset\n",
    "    df_full.reset_index(drop=True, inplace=True)\n",
    "    print(f\"[Info] Processing all {len(df_full)} questions in the dataset.\")\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for model in MODELS:\n",
    "        print(f\"\\n{'='*60}\\n[+] Running VQA with {model} (Full Dataset)\\n{'='*60}\")\n",
    "        \n",
    "        model_results = []\n",
    "        total_valid = 0\n",
    "        total_correct = 0\n",
    "        total_inference_time = 0.0\n",
    "        \n",
    "        for i, row in df_full.iterrows():\n",
    "            # Use the 'image' column as the image ID \n",
    "            image_id = str(row[\"image\"])\n",
    "            image_path = os.path.join(images_dir, image_id)\n",
    "            \n",
    "            # Check if the image file actually exists\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"[Error] Image file not found: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Get question, options, and correct answer\n",
    "            question = str(row[\"question\"])\n",
    "            options = str(row[\"options\"])\n",
    "            correct_answer = str(row[\"answer\"]).strip().upper()\n",
    "\n",
    "            print(f\"\\n[{i+1:03d}] Processing question: {question[:50]}...\")\n",
    "            print(f\"    Image path: {image_path}\")\n",
    "            print(f\"    Correct answer: {correct_answer}\")\n",
    "            \n",
    "            answer_data = generate_vqa_answer(model, image_path, question, options)\n",
    "            \n",
    "            result = {\n",
    "                \"model\": model,\n",
    "                \"question_id\": i + 1,  # Global question number (1 to N)\n",
    "                \"image_id\": image_id,\n",
    "                \"image_path\": image_path,\n",
    "                \"question\": question,\n",
    "                \"options\": options,\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"predicted_answer\": answer_data[\"answer\"],\n",
    "                \"raw_output\": answer_data[\"raw_output\"],\n",
    "                \"inference_time\": answer_data[\"inference_time\"],\n",
    "                \"is_valid\": answer_data[\"is_valid\"]\n",
    "            }\n",
    "            model_results.append(result)\n",
    "            \n",
    "            if answer_data[\"is_valid\"]:\n",
    "                total_valid += 1\n",
    "                total_inference_time += answer_data[\"inference_time\"]\n",
    "                if answer_data[\"answer\"] == correct_answer:\n",
    "                    total_correct += 1\n",
    "                    is_correct = True\n",
    "                else:\n",
    "                    is_correct = False\n",
    "                print(f\"  -> Predicted: '{answer_data['answer']}' (True: {correct_answer}) - {'Correct' if is_correct else 'Incorrect'}\")\n",
    "            else:\n",
    "                print(f\"  -> Invalid prediction: '{answer_data['raw_output']}' (True: {correct_answer})\")\n",
    "\n",
    "            time.sleep(1)  # Small delay\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        reject_rate = (len(model_results) - total_valid) / len(model_results) if len(model_results) > 0 else 0\n",
    "        reject_rate = round(reject_rate, 4)\n",
    "        \n",
    "        accuracy = total_correct / total_valid if total_valid > 0 else 0\n",
    "        accuracy = round(accuracy, 4)\n",
    "        \n",
    "        average_inference_time = total_inference_time / total_valid if total_valid > 0 else 0\n",
    "        average_inference_time = round(average_inference_time, 2)\n",
    "        \n",
    "        # Store results\n",
    "        final_result = {\n",
    "            \"model\": model,\n",
    "            \"subset_range\": \"1-N\",\n",
    "            \"total_questions\": len(df_full),\n",
    "            \"valid_answers\": total_valid,\n",
    "            \"correct_answers\": total_correct,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"reject_rate\": reject_rate,\n",
    "            \"average_inference_time\": average_inference_time,\n",
    "            \"predictions\": model_results\n",
    "        }\n",
    "        results.append(final_result)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n--- Results for {model} (Full Dataset) ---\")\n",
    "        print(f\"Total Questions: {len(df_full)}\")\n",
    "        print(f\"Valid Answers: {total_valid}/{len(df_full)}\")\n",
    "        print(f\"Correct Answers: {total_correct}/{total_valid}\")\n",
    "        print(f\"Accuracy (on valid predictions): {accuracy:.4f}\")\n",
    "        print(f\"Reject Rate: {reject_rate:.4f}\")\n",
    "        print(f\"Average Inference Time: {average_inference_time:.2f} seconds\")\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(\"json_outputs\", exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n[Success] Results for full dataset saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(42)\n",
    "    run_vqa_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c89eaa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating perplexity for story 1 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 1: 2.2\n",
      "\n",
      "Calculating perplexity for story 2 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 2: 2.63\n",
      "\n",
      "Calculating perplexity for story 3 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 3: 1.68\n",
      "\n",
      "Calculating perplexity for story 4 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 4: 2.28\n",
      "\n",
      "Calculating perplexity for story 5 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 5: 2.15\n",
      "\n",
      "Calculating perplexity for story 6 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 6: 2.86\n",
      "\n",
      "Calculating perplexity for story 7 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 7: 2.87\n",
      "\n",
      "Calculating perplexity for story 8 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 8: 2.18\n",
      "\n",
      "Calculating perplexity for story 9 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 9: 2.38\n",
      "\n",
      "Calculating perplexity for story 10 using phi3:3.8b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 10: 2.42\n",
      "\n",
      "Calculating perplexity for story 1 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 1: 2.4\n",
      "\n",
      "Calculating perplexity for story 2 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 2: 3.28\n",
      "\n",
      "Calculating perplexity for story 3 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 3: 3.05\n",
      "\n",
      "Calculating perplexity for story 4 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 4: 2.79\n",
      "\n",
      "Calculating perplexity for story 5 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 5: 3.03\n",
      "\n",
      "Calculating perplexity for story 6 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 6: 4.74\n",
      "\n",
      "Calculating perplexity for story 7 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 7: 2.91\n",
      "\n",
      "Calculating perplexity for story 8 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 8: 3.05\n",
      "\n",
      "Calculating perplexity for story 9 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 9: 2.96\n",
      "\n",
      "Calculating perplexity for story 10 using qwen2.5vl:3b (GGUF)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for story 10: 2.67\n",
      "\n",
      "--- Average Perplexity per Model ---\n",
      "Model phi3:3.8b (GGUF): Average Perplexity = 2.36\n",
      "Model qwen2.5vl:3b (GGUF): Average Perplexity = 3.09\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 4.1 - Perplexity Calculator \n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "import math\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths to the GGUF model files\n",
    "# For phi3, use Ollama path\n",
    "MODEL_PATH_PHI = r\"C:\\Users\\PC\\.ollama\\models\\blobs\\sha256-633fc5be925f9a484b61d6f9b9a78021eeb462100bd557309f01ba84cac26adf\"\n",
    "# For qwen, use alternative model (not Ollama path)\n",
    "MODEL_PATH_QWEN = r\"C:\\Users\\PC\\models\\Qwen2-VL-2B-Instruct-Q4_K_M.gguf\"\n",
    "MMPROJ_PATH_QWEN = r\"C:\\Users\\PC\\models\\mmproj-Qwen2-VL-2B-Instruct-f32.gguf\"\n",
    "\n",
    "# Path to generated stories\n",
    "STORIES_FILE = \"json_outputs/asg_output.json\"\n",
    "# Output file for perplexity results\n",
    "OUTPUT_FILE = \"json_outputs/asg_output_with_perplexity.json\"\n",
    "\n",
    "def calculate_perplexity(model_path, text, n_ctx=2048, is_multimodal=False, mmproj_path=None):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for text using GGUF model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to GGUF model file\n",
    "        text: Text to evaluate\n",
    "        n_ctx: Context window size\n",
    "        is_multimodal: Whether model is multimodal\n",
    "        mmproj_path: Path to mmproj file (for multimodal models)\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity value (lower is better), or inf if fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model with proper configuration\n",
    "        if is_multimodal and mmproj_path:\n",
    "            # For multimodal models, mmproj_path is essential\n",
    "            llm = Llama(\n",
    "                model_path=model_path,\n",
    "                mmproj=mmproj_path,\n",
    "                n_ctx=n_ctx,\n",
    "                n_gpu_layers=-1,\n",
    "                verbose=False,\n",
    "                logits_all=True\n",
    "            )\n",
    "        else:\n",
    "            # For text-only models\n",
    "            llm = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=n_ctx,\n",
    "                n_gpu_layers=-1,\n",
    "                verbose=False,\n",
    "                logits_all=True\n",
    "            )\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = llm.tokenize(text.encode('utf-8'))\n",
    "        if not tokens or len(tokens) < 2:\n",
    "            return float('inf')\n",
    "        \n",
    "        total_log_likelihood = 0.0\n",
    "        token_count = 0\n",
    "        \n",
    "        # Calculate perplexity for each token (limit to 50 tokens)\n",
    "        for i in range(1, min(len(tokens), 50)):\n",
    "            # Context = previous tokens\n",
    "            context = tokens[:i]\n",
    "            \n",
    "            # Convert context tokens to text\n",
    "            context_text = llm.detokenize(context).decode('utf-8', errors='ignore')\n",
    "            \n",
    "            # Get prediction with log probabilities\n",
    "            completion = llm.create_completion(\n",
    "                prompt=context_text,\n",
    "                max_tokens=1,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                logprobs=1,\n",
    "                echo=False,\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            # Extract log probability of actual token\n",
    "            if 'choices' in completion and len(completion['choices']) > 0:\n",
    "                choice = completion['choices'][0]\n",
    "                if 'logprobs' in choice and 'token_logprobs' in choice['logprobs']:\n",
    "                    if len(choice['logprobs']['token_logprobs']) > 0:\n",
    "                        logprob = choice['logprobs']['token_logprobs'][0]\n",
    "                        if not math.isnan(logprob) and logprob != -float('inf'):\n",
    "                            total_log_likelihood += logprob\n",
    "                            token_count += 1\n",
    "        \n",
    "        if token_count == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        avg_neg_log_likelihood = -total_log_likelihood / token_count\n",
    "        perplexity = math.exp(avg_neg_log_likelihood)\n",
    "        return perplexity\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load generated stories\n",
    "    with open(STORIES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        stories_data = json.load(f)\n",
    "    \n",
    "    results_with_ppl = []\n",
    "    \n",
    "    for item in stories_data:\n",
    "        story_text = item.get(\"story\", \"\")\n",
    "        story_id = item.get(\"story_id\", \"unknown\")\n",
    "        model_name_used = item.get(\"model\", \"unknown\")\n",
    "        \n",
    "        if not story_text:\n",
    "            continue\n",
    "            \n",
    "        # Determine which model path to use\n",
    "        model_path = None\n",
    "        mmproj_path = None\n",
    "        is_multimodal = False\n",
    "        evaluator_model_name = \"N/A\"\n",
    "        \n",
    "        if \"phi\" in model_name_used.lower():\n",
    "            model_path = MODEL_PATH_PHI\n",
    "            evaluator_model_name = \"phi3:3.8b (GGUF)\"\n",
    "        elif \"qwen\" in model_name_used.lower():\n",
    "            model_path = MODEL_PATH_QWEN\n",
    "            mmproj_path = MMPROJ_PATH_QWEN\n",
    "            is_multimodal = True\n",
    "            evaluator_model_name = \"qwen2.5vl:3b (GGUF)\"\n",
    "        \n",
    "        if model_path:\n",
    "            print(f\"\\nCalculating perplexity for story {story_id} using {evaluator_model_name}...\")\n",
    "            ppl = calculate_perplexity(\n",
    "                model_path, \n",
    "                story_text,\n",
    "                is_multimodal=is_multimodal,\n",
    "                mmproj_path=mmproj_path\n",
    "            )\n",
    "            \n",
    "            item_with_ppl = item.copy()\n",
    "            item_with_ppl[\"evaluator_model\"] = evaluator_model_name\n",
    "            item_with_ppl[\"perplexity\"] = round(ppl, 2) if not math.isinf(ppl) else float('inf')\n",
    "            results_with_ppl.append(item_with_ppl)\n",
    "            print(f\"Perplexity for story {story_id}: {item_with_ppl['perplexity']}\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_with_ppl, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Calculate average perplexity per model\n",
    "    model_perplexities = {}\n",
    "    for item in results_with_ppl:\n",
    "        model_eval = item.get(\"evaluator_model\", \"N/A\")\n",
    "        ppl = item.get(\"perplexity\", float('inf'))\n",
    "        if model_eval != \"N/A\" and not math.isinf(ppl):\n",
    "            if model_eval not in model_perplexities:\n",
    "                model_perplexities[model_eval] = []\n",
    "            model_perplexities[model_eval].append(ppl)\n",
    "    \n",
    "    print(\"\\n--- Average Perplexity per Model ---\")\n",
    "    for model_eval, ppls in model_perplexities.items():\n",
    "        if ppls:\n",
    "            avg_ppl = sum(ppls) / len(ppls)\n",
    "            print(f\"Model {model_eval}: Average Perplexity = {avg_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67511d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating ROUGE-1 F1 for story 1 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.0842\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 2 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.2184\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 3 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.1707\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 4 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.1834\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 5 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.1417\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 6 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.1453\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 7 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.1582\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 8 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.1841\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 9 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.1139\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 10 using phi3:3.8b...\n",
      "  -> ROUGE-1 F1: 0.3954\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 1 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.1881\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 2 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.1308\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 3 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.3161\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 4 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.4332\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 5 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.1643\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 6 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.3810\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 7 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.1784\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 8 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.2158\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 9 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.3063\n",
      "\n",
      "Calculating ROUGE-1 F1 for story 10 using qwen2.5vl:3b...\n",
      "  -> ROUGE-1 F1: 0.1344\n",
      "\n",
      "[SUCCESS] ROUGE evaluation results saved to json_outputs/ats_rouge_results.json\n",
      "\n",
      "--- Average ROUGE-1 F1 per Model ---\n",
      "  Model phi3:3.8b: Average ROUGE-1 F1 = 0.1795\n",
      "  Model qwen2.5vl:3b: Average ROUGE-1 F1 = 0.2448\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 4.2 - ROUGE-1 F1 Evaluation for Abstractive Text Summarization\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the summarization output\n",
    "SUMMARIES_FILE = \"json_outputs/ats_output.json\"\n",
    "# Output file for ROUGE results\n",
    "OUTPUT_FILE = \"json_outputs/ats_rouge_results.json\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text for ROUGE calculation by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation\n",
    "    - Tokenizing\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize and rejoin\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def calculate_rouge_scores(hypotheses, references):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores between hypotheses and references.\n",
    "    \n",
    "    Args:\n",
    "        hypotheses (list): List of generated summaries\n",
    "        references (list): List of original stories\n",
    "        \n",
    "    Returns:\n",
    "        dict: ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L)\n",
    "    \"\"\"\n",
    "    # Initialize ROUGE evaluator\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def calculate_rouge1_f1_for_summary(summary, original_story):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE-1 F1 score for a single summary.\n",
    "    \n",
    "    Args:\n",
    "        summary (str): Generated summary\n",
    "        original_story (str): Original story text\n",
    "        \n",
    "    Returns:\n",
    "        float: ROUGE-1 F1 score\n",
    "    \"\"\"\n",
    "    if not summary or not original_story:\n",
    "        return 0.0\n",
    "    \n",
    "    # Preprocess texts\n",
    "    summary_clean = preprocess_text(summary)\n",
    "    story_clean = preprocess_text(original_story)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(summary_clean, story_clean)\n",
    "    \n",
    "    # Return ROUGE-1 F1 score\n",
    "    return scores[0]['rouge-1']['f']\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to evaluate summarization results with ROUGE-1 F1\"\"\"\n",
    "    # Ensure NLTK data is available\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    # Load summarization results\n",
    "    if not os.path.exists(SUMMARIES_FILE):\n",
    "        print(f\"[ERROR] Summarization results file not found at {SUMMARIES_FILE}\")\n",
    "        print(\"Please run Part 3.2 (ATS) first to generate summaries.\")\n",
    "        return\n",
    "    \n",
    "    with open(SUMMARIES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        summaries_data = json.load(f)\n",
    "    \n",
    "    results_with_rouge = []\n",
    "    \n",
    "    # Process each summary\n",
    "    for item in summaries_data:\n",
    "        summary = item.get(\"summary\", \"\")\n",
    "        original_story = item.get(\"original_story\", \"\")\n",
    "        model_name = item.get(\"original_model\", \"unknown\")\n",
    "        story_id = item.get(\"story_id\", \"unknown\")\n",
    "        \n",
    "        if not summary or not original_story:\n",
    "            print(f\"[WARNING] Missing summary or story for {story_id}. Skipping ROUGE calculation.\")\n",
    "            rouge1_f1 = 0.0\n",
    "            is_valid = False\n",
    "        else:\n",
    "            print(f\"\\nCalculating ROUGE-1 F1 for story {story_id} using {model_name}...\")\n",
    "            rouge1_f1 = calculate_rouge1_f1_for_summary(summary, original_story)\n",
    "            is_valid = True\n",
    "            print(f\"  -> ROUGE-1 F1: {rouge1_f1:.4f}\")\n",
    "        \n",
    "        # Create result entry\n",
    "        result = item.copy()\n",
    "        result[\"rouge1_f1\"] = round(rouge1_f1, 4)\n",
    "        result[\"is_valid\"] = is_valid\n",
    "        results_with_rouge.append(result)\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results_with_rouge, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n[SUCCESS] ROUGE evaluation results saved to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Calculate average ROUGE-1 F1 per model\n",
    "    model_scores = {}\n",
    "    for item in results_with_rouge:\n",
    "        model = item.get(\"original_model\", \"N/A\")\n",
    "        score = item.get(\"rouge1_f1\", 0.0)\n",
    "        if model != \"N/A\" and item.get(\"is_valid\", False):\n",
    "            if model not in model_scores:\n",
    "                model_scores[model] = []\n",
    "            model_scores[model].append(score)\n",
    "    \n",
    "    print(\"\\n--- Average ROUGE-1 F1 per Model ---\")\n",
    "    for model, scores in model_scores.items():\n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            print(f\"  Model {model}: Average ROUGE-1 F1 = {avg_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Model {model}: No valid ROUGE scores calculated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718da19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLI Classification Accuracy Results ---\n",
      "\n",
      "Model: phi3:3.8b\n",
      "Accuracy: 0.7900 (79/100)\n",
      "\n",
      "Model: qwen2.5vl:3b\n",
      "Accuracy: 0.8300 (83/100)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 4.3 - Classification Accuracy Evaluation for Natural Language Inference\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the NLI prediction outputs\n",
    "NLI_OUTPUT_PHI = \"json_outputs/nli_output_phi3.json\"\n",
    "NLI_OUTPUT_QWEN = \"json_outputs/nli_output_qwen.json\"\n",
    "# Output file for accuracy results\n",
    "OUTPUT_FILE = \"json_outputs/nli_accuracy_results.json\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to evaluate NLI classification accuracy\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Process phi3:3.8b results\n",
    "    if os.path.exists(NLI_OUTPUT_PHI):\n",
    "        with open(NLI_OUTPUT_PHI, \"r\", encoding=\"utf-8\") as f:\n",
    "            phi3_data = json.load(f)\n",
    "        \n",
    "        # phi3_data is a list with one item (the results for phi3)\n",
    "        if len(phi3_data) > 0:\n",
    "            phi3_result = phi3_data[0]\n",
    "            accuracy = phi3_result[\"accuracy\"]\n",
    "            correct_predictions = phi3_result[\"correct_predictions\"]\n",
    "            total_predictions = phi3_result[\"total_items\"]\n",
    "            \n",
    "            results[\"phi3:3.8b\"] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"correct_predictions\": correct_predictions,\n",
    "                \"total_predictions\": total_predictions\n",
    "            }\n",
    "    \n",
    "    # Process qwen2.5vl:3b results\n",
    "    if os.path.exists(NLI_OUTPUT_QWEN):\n",
    "        with open(NLI_OUTPUT_QWEN, \"r\", encoding=\"utf-8\") as f:\n",
    "            qwen_data = json.load(f)\n",
    "        \n",
    "        # qwen_data is a list with one item (the results for qwen)\n",
    "        if len(qwen_data) > 0:\n",
    "            qwen_result = qwen_data[0]\n",
    "            accuracy = qwen_result[\"accuracy\"]\n",
    "            correct_predictions = qwen_result[\"correct_predictions\"]\n",
    "            total_predictions = qwen_result[\"total_items\"]\n",
    "            \n",
    "            results[\"qwen2.5vl:3b\"] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"correct_predictions\": correct_predictions,\n",
    "                \"total_predictions\": total_predictions\n",
    "            }\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- NLI Classification Accuracy Results ---\")\n",
    "    for model, metrics in results.items():\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f} ({metrics['correct_predictions']}/{metrics['total_predictions']})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756a54eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CIDEr Evaluation Results for Image Captioning ---\n",
      "\n",
      "Model: qwen2.5vl:3b\n",
      "CIDEr Score: 0.6318\n",
      "Valid Captions: 100/100\n",
      "Reject Rate: 0.0000\n",
      "Average Inference Time: 146.74 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 4.4 - CIDEr Evaluation for Image Captioning\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the combined image captioning output (from the merge script)\n",
    "IMAGE_CAPTIONS_FILE = \"json_outputs/ic_output.json\"\n",
    "# Output file for CIDEr results (simplified format for Part 4.4)\n",
    "OUTPUT_FILE = \"json_outputs/ic_cider_results.json\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to extract and present CIDEr results from Part 3.4 output\"\"\"\n",
    "    # Check if the combined results file exists\n",
    "    if not os.path.exists(IMAGE_CAPTIONS_FILE):\n",
    "        print(f\"[ERROR] Combined image captioning results file not found at {IMAGE_CAPTIONS_FILE}\")\n",
    "        print(\"Please run the merge script first to combine all subsets.\")\n",
    "        print(\"Expected file structure: json_outputs/ic_output.json\")\n",
    "        return\n",
    "    \n",
    "    # Load the combined results\n",
    "    with open(IMAGE_CAPTIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        caption_data = json.load(f)\n",
    "    \n",
    "    # Extract CIDEr results\n",
    "    results = {}\n",
    "    \n",
    "    # Assuming there's only one model in the results (qwen2.5vl:3b)\n",
    "    if caption_data and len(caption_data) > 0:\n",
    "        model_data = caption_data[0]\n",
    "        model_name = model_data[\"model\"]\n",
    "        \n",
    "        results[model_name] = {\n",
    "            \"cider_score\": model_data[\"cider_score\"],\n",
    "            \"total_images\": model_data[\"total_images\"],\n",
    "            \"valid_captions\": model_data[\"valid_captions\"],\n",
    "            \"reject_rate\": model_data[\"reject_rate\"],\n",
    "            \"average_inference_time\": model_data[\"average_inference_time\"]\n",
    "        }\n",
    "    \n",
    "    # Save simplified results\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n--- CIDEr Evaluation Results for Image Captioning ---\")\n",
    "    for model, metrics in results.items():\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(f\"CIDEr Score: {metrics['cider_score']:.4f}\")\n",
    "        print(f\"Valid Captions: {metrics['valid_captions']}/{metrics['total_images']}\")\n",
    "        print(f\"Reject Rate: {metrics['reject_rate']:.4f}\")\n",
    "        print(f\"Average Inference Time: {metrics['average_inference_time']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa637c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "VQA Evaluation Metrics (Questions 11-40)\n",
      "==========================================================================================\n",
      "Total Qs        Valid Ans       Correct Ans     Accuracy        Reject Rate     Avg Time (s)   \n",
      "------------------------------------------------------------------------------------------\n",
      "40              40              14              0.3500          0.0000          402.93         \n",
      "==========================================================================================\n",
      "\n",
      "[Success] Combined results saved to json_outputs/results/vqa_results.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 4.5 - Visual question answering\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_vqa_results(file_path):\n",
    "    \"\"\"Load VQA results from a JSON file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"[Error] File not found: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            return data[0]  # Return the first (and only) model's results\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_combined_metrics(results_list):\n",
    "    \"\"\"Calculate combined metrics for multiple result sets.\"\"\"\n",
    "    total_questions = 0\n",
    "    total_valid = 0\n",
    "    total_correct = 0\n",
    "    total_inference_time = 0.0\n",
    "    \n",
    "    for results in results_list:\n",
    "        total_questions += results[\"total_questions\"]\n",
    "        total_valid += results[\"valid_answers\"]\n",
    "        total_correct += results[\"correct_answers\"]\n",
    "        total_inference_time += results[\"average_inference_time\"] * results[\"valid_answers\"]\n",
    "    \n",
    "    # Calculate combined metrics\n",
    "    combined_accuracy = total_correct / total_valid if total_valid > 0 else 0\n",
    "    combined_reject_rate = (total_questions - total_valid) / total_questions if total_questions > 0 else 0\n",
    "    combined_avg_inference_time = total_inference_time / total_valid if total_valid > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"model\": \"qwen2.5vl:3b\",\n",
    "        \"subset_range\": \"11-40\",\n",
    "        \"total_questions\": total_questions,\n",
    "        \"valid_answers\": total_valid,\n",
    "        \"correct_answers\": total_correct,\n",
    "        \"accuracy\": combined_accuracy,\n",
    "        \"reject_rate\": combined_reject_rate,\n",
    "        \"average_inference_time\": combined_avg_inference_time\n",
    "    }\n",
    "\n",
    "def print_table(metrics):\n",
    "    \"\"\"Print metrics in a table format with headers.\"\"\"\n",
    "    # Print header\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15} {:<15}\".format(\n",
    "        \"Total Qs\", \"Valid Ans\", \"Correct Ans\", \"Accuracy\", \"Reject Rate\", \"Avg Time (s)\"\n",
    "    ))\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # Print values\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15.4f} {:<15.4f} {:<15.2f}\".format(\n",
    "        metrics[\"total_questions\"],\n",
    "        metrics[\"valid_answers\"],\n",
    "        metrics[\"correct_answers\"],\n",
    "        metrics[\"accuracy\"],\n",
    "        metrics[\"reject_rate\"],\n",
    "        metrics[\"average_inference_time\"]\n",
    "    ))\n",
    "\n",
    "def save_combined_results(output_file, combined_metrics):\n",
    "    \"\"\"Save combined results to a JSON file.\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Prepare the result structure\n",
    "    result = {\n",
    "        \"model\": combined_metrics[\"model\"],\n",
    "        \"subset_range\": combined_metrics[\"subset_range\"],\n",
    "        \"total_questions\": combined_metrics[\"total_questions\"],\n",
    "        \"valid_answers\": combined_metrics[\"valid_answers\"],\n",
    "        \"correct_answers\": combined_metrics[\"correct_answers\"],\n",
    "        \"accuracy\": combined_metrics[\"accuracy\"],\n",
    "        \"reject_rate\": combined_metrics[\"reject_rate\"],\n",
    "        \"average_inference_time\": combined_metrics[\"average_inference_time\"]\n",
    "    }\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n[Success] Combined results saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # List of JSON files to process (based on your directory structure)\n",
    "    json_files = [\n",
    "        \"json_outputs/vqa_outputs/vqa_output_subset_11_20.json\",\n",
    "        \"json_outputs/vqa_outputs/vqa_output_subset_21_30.json\",\n",
    "        \"json_outputs/vqa_outputs/vqa_output_subset_31_40.json\",\n",
    "        \"json_outputs/vqa_outputs/vqa_output_subset_70_80.json\"\n",
    "    ]\n",
    "    \n",
    "    # Load results from each file\n",
    "    results_list = []\n",
    "    for file in json_files:\n",
    "        results = load_vqa_results(file)\n",
    "        if results:\n",
    "            results_list.append(results)\n",
    "    \n",
    "    # Calculate and print table\n",
    "    if results_list:\n",
    "        combined_metrics = calculate_combined_metrics(results_list)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(\"VQA Evaluation Metrics (Questions 11-40)\")\n",
    "        print(\"=\" * 90)\n",
    "        print_table(combined_metrics)\n",
    "        print(\"=\" * 90)\n",
    "        \n",
    "        # Save combined results to a new JSON file\n",
    "        save_combined_results(\"json_outputs/results/vqa_results.json\", combined_metrics)\n",
    "    else:\n",
    "        print(\"[Error] No valid results to combine.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
